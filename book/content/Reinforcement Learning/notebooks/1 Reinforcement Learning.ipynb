{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### From Unsupervised to Supervised Learning\n",
    "\n",
    "Unsupervised learning allows a model to discover structure present in the environment. This is a powerful way to build up a *neutral* representation of the world. But a representation generated by unsupervised learning does not have any valence (positive or negative value) - it does not tell you what things in the world are good or bad for you. Such a representation might help you infer what to expect,but it does not tell you what to do.  We need a way to learn how to behave - how to make better decisions and act to optimize rewards (increase positive outcomes, decrease negative outcomes).\n",
    "\n",
    "Attaching value to things in the world, and making decisions that can arrive at these values introduces a key opportunity for training.  Supervised learning introduces the concepts of correct and incorrect, better and worse. In supervised learning the ends alter the means; the valenced outcome can reach back to change the processing that lead up to that outcome -- and when this learning works, it leads to better future decisions and outcomes.                       \n",
    "\n",
    "\n",
    "## Predictions are Hard\n",
    "\n",
    "At first pass, the only way to make accurate predictions is to travel backward in time. When you first receive a chocolate icecream cone your entire life lead up to this momentous occasion - what part of your life should be credited as its predictor?  Less extravagantly, even the few minutes before getting the icecream your natural environment contained an incredible amount of information and your brain cycled through a vast number of different states.  What feature of the environment might guide you to future icecream cones?  Which stimuli and brain states were the predictors?  Figuring this out is known as the *credit assignment problem*; it's an extremely hard problem to solve, and we'll return to it repeatedly in this course.\n",
    "\n",
    "In order to survive, animals need to pursue rewarding things like food and avoid harmful things like getting attacked by other animals. Any ability to predict reward or punishment is generally helpful. Prediction requires associating a reward or punishment with things that consistently precede it.  If seeing over-ripe fruit on the ground is correlated with later finding ripe fruit above in the tree, the neutral stimulus of rotting fruit on the ground should teach you to look up for a rewarding piece of ripe fruit. (In behaviorist jargon, the rotting fruit on the ground begins as a neutral stimulus, looking up begins as an unconditioned response and the fruit in the tree is an unconditioned stimulus -- after learning, the rotting fruit is a conditioned stimulus and looking up is a conditioned response.) If an animal growling is correlated with that animal then attacking, hearing growling should make you prepare for an attack.  \n",
    "\n",
    "How might animals learn to accurately predict rewards? Neural activity associated with a reward could gradually become associated with stimuli that precede the reward. Remember that with Hebbian Learning \"neurons that fire together, wire together\" and this can operate over both **spatial and temporal proximity**. Temporal proximity can potentially give us prediction. If a small red circle always appears right before a large chocolate icecream cone, the reward signal produced by the icecream cone could eventually become correlated with the small red circle.   \n",
    "        \n",
    "\n",
    "## Temporal Difference Learning\n",
    "\n",
    "Temporal difference learning is a method that facilitates the backward propagation of predictions through a temporal representation. While it does not literally enable predictions to travel backward in time, it effectively allows them to update prior estimates based on subsequent outcomes, refining the temporal representation of events.\n",
    "\n",
    "We start with a temporal sequence of stimuli: A->B->C->D. Let's call stimuli distributed over time *events*. Hebbian learning allows us to build up a neutral representation of this temporal sequence of events. Now, at time point D you get a reward. We want to associate the earliest event that reliably predicts this reward with the reward. Temporal difference learning is a way to transfer the prediction backward from D to C, from C to B, and ultimately from B to A.  \n",
    "\n",
    "\n",
    "The key to understanding temporal difference learning is to shift from viewing time as a chronological construct to conceptualizing it as a linear sequence of events. This linear representation consists of stimulus events interspersed with intervals of waiting. If we imagine time flowing from left (past) to right (future), the objective is to enable information about rewards to propagate from right (future) to left (past) in order to generate accurate predictions.\n",
    "\n",
    "\n",
    "### Temporal Difference Learning from Prediction Errors \n",
    "In this lab, we will be recreating the model of mesolimbic dopamine cell activity during monkey conditioning proposed by [Montague, Dayan, and Sejnowski in their 1996 paper](http://www.jneurosci.org/content/jneuro/16/5/1936.full.pdf). This model compares temporal difference learning from prediction errors to physiological data. We'll use this model to replicate Figure 5 from the paper.\n",
    "\n",
    "\n",
    "We'll start by importing Numpy, Matplotlib, and PsyNeuLink, and the 3D toolkit from Matplotlib.\n",
    "\n",
    "\n",
    "**Note**: *Some of the PsyNeuLink syntax in this notebook is a little different than the current syntax that you've become familiar with over the previous two labs.  PNL is under active developmen and the TD learning exercises that work so well in this notebook are not ironed out in the current branch of PNL, so we're installing an older version of PNL in the cell below. Your lab instructor will help you navigate these minor differences.  A couple things to be aware of with this older syntax is that Composition (new syntax) = System (old syntax); an intermediate structure that linked together mechanisms to support learning was called a Process (old syntax).*\n"
   ],
   "id": "79976b138d1f771"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Code Examples\n",
    "\n",
    "### Installation & Setup\n",
    "\n"
   ],
   "id": "5c13b1e33b3c5c1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install psyneulink"
   ],
   "id": "7e841b3f94d34a0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import psyneulink as pnl\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ],
   "id": "f3d6b6fe661fd32d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Temporal Difference Learning from Prediction Errors \n",
    "In this lab, we will be recreating the model of mesolimbic dopamine cell activity during monkey conditioning proposed by [Montague, Dayan, and Sejnowski in their 1996 paper](http://www.jneurosci.org/content/jneuro/16/5/1936.full.pdf). This model compares temporal difference learning from prediction errors to physiological data. We'll use this model to replicate Figure 5 from the paper.\n",
    "\n",
    "![Montague et al., 1996, Figure 5](https://younesstrittmatter.github.io/502B/_static/images/montague_fig_5.jpg)\n"
   ],
   "id": "31b87a96e9cf6b93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "// TODO: Eventually delete:\n",
    "**Note**: *Some of the PsyNeuLink syntax in this notebook is a little different than the current syntax that you've become familiar with over the previous two labs.  PNL is under active developmen and the TD learning exercises that work so well in this notebook are not ironed out in the current branch of PNL, so we're installing an older version of PNL in the cell below. Your lab instructor will help you navigate these minor differences.  A couple things to be aware of with this older syntax is that Composition (new syntax) = System (old syntax); an intermediate structure that linked together mechanisms to support learning was called a Process (old syntax).*\n"
   ],
   "id": "2442768b0d59dd14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
