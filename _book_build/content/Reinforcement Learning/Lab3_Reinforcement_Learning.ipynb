{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Lab3_Reinforcement_Learning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpa8nQwHwQGR"
   },
   "source": [
    "# Lab 3: Temporal Difference Learning\n",
    "\n",
    "\n",
    "## From Unsupervised to Supervised Learning\n",
    "\n",
    "Unsupervised learning allows a model to discover structure present in the environment.  This is a powerful way to build up a *neutral* representation of the world.  But a represntation generated by unsupervised learning does not have any valence (positive or negative value) -- it does not tell you what things in the world are good or bad for you.  Such a representation might help you infer what to expect, but it does not tell you what to do.  We need a way to learn how to behave -- how to make better decisions and act to optimize rewards (increase positive outcomes, decrease negative outcomes).  \n",
    "\n",
    "Attaching value to things in the world, and making decisions that can arrive at these values introduces a key opportunity for training.  Supervised learning introduces the concepts of correct and incorrect, better and worse.  In supervised learning the ends alter the means; the valenced outcome can reach back to change the processing that lead up to that outcome -- and when this learning works, it leads to better future decisions and outcomes.                         \n",
    "\n",
    "\n",
    "## Predictions are Hard\n",
    "\n",
    "At first pass, the only way to make accurate predictions is to travel backward in time.  When you first receive a chocolate icecream cone your entire life lead up to this momentous occasion -- what part of your life should be credited as its predictor?  Less extravagantly, even the few minutes before getting the icecream your natural environment contained an incredible amount of information and your brain cycled through a vast number of different states.  What feature of the environment might guide you to future icecream cones?  Which stimuli and brain states were the predictors?  Figuring this out is known as the credit assignment problem; it's an extremely hard problem to solve and we'll return to it repeatedly this semester.\n",
    "\n",
    "In order to survive, animals need to pursue rewarding things like food and avoid harmful things like getting attacked by other animals.  Any ability to predict reward or punishment is generally helpful.  Prediction requires associating a reward or punishment with things that consistently precede it.  If seeing over-ripe fruit on the ground is correlated with later finding ripe fruit above in the tree, the neutral stimulus of rotting fruit on the ground should teach you to look up for a rewarding piece of ripe fruit.  (In behaviorist jargon, the rotting fruit on the ground begins as a neutral stimulus, looking up begins as an unconditioned response and the fruit in the tree is an unconditioned stimulus -- after learning, the rotting fruit is a conditioned stimulus and looking up is a conditioned response.)  If an animal growling is correlated with that animal then attacking, hearing growling should make you prepare for an attack.  \n",
    "\n",
    "How might animals learn to accurately predict rewards?  Neural activity associated with a reward could gradually become associated with stimuli that precede the reward.  Remember that with Hebbian Learning \"neurons that fire together, wire together\" and this can operate over both **spatial & temporal proximity**.  Temporal proximity can potentially give us prediction.  If a small red circle always appears right before a large chocolate icecream cone, the reward signal produced by the icecream cone could eventually become correlated with the small red circle.   \n",
    "        \n",
    "\n",
    "## Temporal Difference Learning\n",
    "\n",
    "Temporal difference learning is a method that allows predictions to travel backward in time.  Okay, not exactly, but kind of -- **it allows predictions to propogate backward in our representation of time.**  \n",
    "\n",
    "We start with a temporal sequence of stimuli: A->B->C->D.  Let's call stimuli distributed over time *events*.  Hebbian learning allows us to build up a neutral representation of this temporal sequence of events.  Now at timepoint D you get a reward.  We want to associate the earliest event that reliably predicts this reward with the reward.  Temporal difference learning is a way to transfer the prediction backward from D to C, from C to B, and ultimately from B to A.  \n",
    "\n",
    "The key to understanding temporal difference learning is to stop thinking about time as time, and start thinking about time as a linear object.  The line is a sequence of stimulus events and waiting (between events).  If you imagine time flows from left (past) to right (future), we want information about rewards to flow from right (future) to left (past) to create predictions.  \n",
    "\n",
    "\n",
    "### Temporal Difference Learning from Prediction Errors \n",
    "In this lab, we will be recreating the model of mesolimbic dopamine cell activity during monkey conditioning proposed by [Montague, Dayan, and Sejnowski in their 1996 paper](http://www.jneurosci.org/content/jneuro/16/5/1936.full.pdf). This model compares temporal difference learning from prediction errors to physiological data. We'll use this model to replicate Figure 5 from the paper.\n",
    "\n",
    "\n",
    "We'll start by importing Numpy, Matplotlib, and PsyNeuLink, and the 3D toolkit from Matplotlib.\n",
    "\n",
    "\n",
    "**Note**: *Some of the PsyNeuLink syntax in this notebook is a little different than the current syntax that you've become familiar with over the previous two labs.  PNL is under active developmen and the TD learning exercises that work so well in this notebook are not ironed out in the current branch of PNL, so we're installing an older version of PNL in the cell below. Your lab instructor will help you navigate these minor differences.  A couple things to be aware of with this older syntax is that Composition (new syntax) = System (old syntax); an intermediate structure that linked together mechanisms to support learning was called a Process (old syntax).*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRtHoeCR29dG",
    "outputId": "44371dfb-0843-45d3-c9e7-1738e499e0da"
   },
   "source": [
    "# uncomment to install psyneulink\n",
    "!pip install psyneulink"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "FwN8EDs9wQGS",
    "outputId": "4ea23655-b21f-49fd-b47f-3e3067ad883a"
   },
   "source": [
    "import numpy as np\n",
    "import psyneulink as pnl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns \n",
    "\n",
    "%autosave 5\n",
    "%matplotlib inline\n",
    "\n",
    "print(pnl.__version__)\n",
    "\n",
    "np.random.seed(0)\n",
    "sns.set(style='white', context='talk', palette=\"colorblind\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUNCCJyRwQGW"
   },
   "source": [
    "Figure 5A is a \"plot of \u2202(t) over time for three trials during training (1, 30, and 50)\", where $\u2202(t) = r(t) + w(t)x(t) - w(t - 1)x(t - 1)$ is the difference between the stimulus value and the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO4Xdd3bwQGc"
   },
   "source": [
    "For this experiment, the stimulus (a light) starts at timestep 41 and continues for the remainder of the trial, and the reward is delivered at timestep 54, so we'll create variables to represent that.\n",
    "\n",
    "Next, we'll create the arrays that represent the samples and targets. The sample values in this experiment stay the same across all trials, but the targets do not; every 15 trials, the reward was withheld."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PtuclQrNwQGh"
   },
   "source": [
    "def build_stimulus(n_trials, n_time_steps, no_reward_trials, stimulus_onset=41, reward_delivery=54):\n",
    "\n",
    "    samples = []\n",
    "    targets = []\n",
    "    for trial in range(n_trials):\n",
    "        target = [0.] * n_time_steps\n",
    "        target[reward_delivery] = 1.\n",
    "        if trial in no_reward_trials:\n",
    "            target[reward_delivery] = 0.\n",
    "        targets.append(target)\n",
    "\n",
    "        sample = [0.] * n_time_steps\n",
    "        for i in range(stimulus_onset, n_time_steps):\n",
    "            sample[i] = 1.\n",
    "        samples.append(sample)\n",
    "\n",
    "    return np.array(samples), np.array(targets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ea-JvJ1nLxHM"
   },
   "source": [
    "n_time_steps = 60\n",
    "n_trials = 120\n",
    "no_reward_trials = {14, 29, 44, 59, 74, 89}\n",
    "samples, targets = build_stimulus(n_trials, n_time_steps, no_reward_trials)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmOrdAivwQGj"
   },
   "source": [
    "The following 3 cells are included simply to view (inspect) elements that were created in the previous cell.  Whenever you aren't certain what some line of code is doing, try creating a new cell and executing parts of the code that are not yet clear.   "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4wIBfQuwQGk",
    "outputId": "bc1d27dc-1c95-4ab8-8859-b6e48edadc2a"
   },
   "source": [
    "\"\"\"\n",
    "inspect: \n",
    "\n",
    "samples represents the absence [0] and presence [1] \n",
    "of a stimulus over 120 units of time.\n",
    "\n",
    "targets represents the absence [0] and presence [1] \n",
    "of a reward over 120 units of time. \n",
    "\"\"\"\n",
    "\n",
    "trial_id = 0 \n",
    "print('samples:\\n', samples[trial_id,])\n",
    "print('targets:\\n', targets[trial_id,])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "ZcjzntclS5EO",
    "outputId": "72e7115b-9482-4200-fd9d-b21eda63c2b0"
   },
   "source": [
    "trial_id = 0\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(11,4))\n",
    "ax.plot(samples[trial_id,], label='stimulus')\n",
    "ax.plot(targets[trial_id,], label='reward delivery')\n",
    "\n",
    "ax.set_title(f'Input and target values over time, trial id = {trial_id}')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_xlabel('Time')\n",
    "f.legend(bbox_to_anchor=(.5,.7),frameon=False)\n",
    "sns.despine()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Ab8WyR41S5ET",
    "outputId": "2f6b9dec-a880-4763-b5e9-ab9e1f4c2114"
   },
   "source": [
    "f, axes = plt.subplots(2, 1, figsize=(10,6), sharex=True)\n",
    "\n",
    "axes[0].imshow(samples, cmap='viridis', aspect='auto')\n",
    "axes[0].set_ylabel('Trial Number')\n",
    "axes[0].set_title('Stimulus')\n",
    "\n",
    "axes[1].imshow(targets, cmap='viridis', aspect='auto')\n",
    "axes[1].set_ylabel('Trial Number')\n",
    "axes[1].set_title('Reward')\n",
    "axes[1].set_xlabel('Time')\n",
    "f.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yj4WOC3SwQGu",
    "outputId": "8f37e6ff-5287-4796-aff8-cb36f8c9ea61"
   },
   "source": [
    "# inspect: We have 120 trials, each with 120 units of time, \n",
    "print('targets shape: ', np.shape(targets))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu5KuocqwQHA"
   },
   "source": [
    "Now we'll set up the input values for the samples and the targets. We'll put the `samples` and `targets` variables in dictionaries assigned to the `sample` and `action_selection` mechanisms, respectively. Then we'll create the System in which the Process will run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yzsiEYxoLxHO"
   },
   "source": [
    "def get_model(composition_name='TD', learning_rate=.3):\n",
    "    \"\"\"get a model, described in Montague, Dayan, and Sejnowski (1996)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    composition_name : str\n",
    "        the name of the composition\n",
    "    learning_rate : float\n",
    "        learning rate, default to .3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pnl.composition, list\n",
    "        the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Create Processing Components\n",
    "    sample_mechanism = pnl.TransferMechanism(default_variable=np.zeros(60),\n",
    "                                              name=pnl.SAMPLE)\n",
    "\n",
    "    action_selection = pnl.TransferMechanism(default_variable=np.zeros(60),\n",
    "                                              function=pnl.Linear(slope=1.0,\n",
    "                                                                  intercept=0.01),\n",
    "                                              name='Action Selection')\n",
    "\n",
    "    sample_to_action_selection = pnl.MappingProjection(sender=sample_mechanism,\n",
    "                                                        receiver=action_selection,\n",
    "                                                        matrix=np.zeros((60, 60)))\n",
    "    # Create Composition\n",
    "    composition_name = 'TD_Learning_Figure_5A'\n",
    "    comp = pnl.Composition(name=composition_name)\n",
    "\n",
    "    # Add Processing Components to the Composition\n",
    "    pathway = [sample_mechanism, sample_to_action_selection, action_selection]\n",
    "\n",
    "    # Add Learning Components to the Composition\n",
    "    learning_related_components = comp.add_td_learning_pathway(pathway, learning_rate).learning_components\n",
    "\n",
    "    # Unpack Relevant Learning Components\n",
    "    prediction_error_mechanism = learning_related_components[pnl.OBJECTIVE_MECHANISM]\n",
    "    target_mechanism = learning_related_components[pnl.TARGET_MECHANISM]\n",
    "\n",
    "    # Create Log\n",
    "    prediction_error_mechanism.log.set_log_conditions(pnl.VALUE)\n",
    "\n",
    "    nodes = [sample_mechanism, prediction_error_mechanism, target_mechanism]\n",
    "\n",
    "\n",
    "    return comp, nodes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FXrkgXN3LxHO",
    "outputId": "6d426356-67f6-4f63-e7f7-357df7d9f8eb"
   },
   "source": [
    "comp, nodes = get_model()\n",
    "[sample_mechanism, prediction_error_mechanism, target_mechanism] = nodes\n",
    "comp.show_graph(output_fmt='jupyter')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJ3Tc572sfwj",
    "outputId": "c1a6a4bf-f064-4637-9f81-6847c4cb2cec"
   },
   "source": [
    "samples, targets = build_stimulus(n_trials, n_time_steps, no_reward_trials)\n",
    "\n",
    "input_dict = {\n",
    "    sample_mechanism: samples,\n",
    "    target_mechanism: targets\n",
    "}\n",
    "\n",
    "comp.learn(inputs=input_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL2lUGvpwQHF"
   },
   "source": [
    "Now we can create the plot"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZT5jQA1iLxHP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "outputId": "d6b07e01-d45d-4494-9d3d-06590247cf32"
   },
   "source": [
    "# Get Delta Values from Log\n",
    "delta_vals = prediction_error_mechanism.log.nparray_dictionary()[\n",
    "    comp.name][pnl.VALUE]\n",
    "\n",
    "# Plot Delta Values form trials 1, 30, and 50\n",
    "t_start = 35\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "plt.plot(delta_vals[0][0], \"-o\", label=\"Trial 1\")\n",
    "plt.plot(delta_vals[29][0], \"-s\", label=\"Trial 30\")\n",
    "plt.plot(delta_vals[49][0], \"-o\", label=\"Trial 50\")\n",
    "plt.title(\"Montague et. al. (1996) -- Figure 5A\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(r\"$\\delta$\")\n",
    "plt.legend(frameon=False)\n",
    "plt.xlim([t_start, n_time_steps])\n",
    "plt.xticks()\n",
    "fig.tight_layout()\n",
    "sns.despine()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tneTuoKwQHN"
   },
   "source": [
    "### Lab Exercise 1a\n",
    "Write a detailed caption for the figure created above.  You can refer to the original paper, but the caption you write must be your own words.  Your caption should both describe and interpret the information depicted, such that the figure plus your caption can \"stand alone\" and make sense to someone who has not read the paper.      \n",
    "\n",
    "### Lab Exercise 1b\n",
    "Scroll back up to the cell that executes the model and quickly scan over all the delta values on each trial.  Note that trial 0 in the printed output is depicted as trial 1 in the graph above (because Python indexes starting at 0).  At what point has the prediction migrated to the onset of the stimulus with a value above 0.5?    \n",
    "\n",
    "\n",
    "\n",
    "## 3D Plot, Figure 5B\n",
    "\n",
    "Now we'll create a 3D plot showing the entire time course of the experiment. According to the paper, training began at trial 10, so we'll edit the targets array to match and run the system again.  \n",
    "\n",
    "(Again, it will take a minute or two to complete running.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5tz3FZoeLxHQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1faa483-16d8-4b61-df81-fe0d6b36c5c6"
   },
   "source": [
    "comp, nodes = get_model()\n",
    "[sample_mechanism, prediction_error_mechanism, target_mechanism] = nodes\n",
    "\n",
    "# Create Stimulus Dictionary\n",
    "n_time_steps = 60\n",
    "n_trials = 120\n",
    "no_reward_trials = {\n",
    "    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 29, 44, 59, 74, 89, 104, 119\n",
    "}\n",
    "samples, targets = build_stimulus(n_trials, n_time_steps, no_reward_trials)\n",
    "\n",
    "\n",
    "input_dict = {\n",
    "    sample_mechanism: samples,\n",
    "    target_mechanism: targets\n",
    "}\n",
    "\n",
    "comp.learn(inputs=input_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zlCCnNz3LxHQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "outputId": "5820cccf-6647-42b6-c4f4-dc6c3f921dd3"
   },
   "source": [
    "f, axes = plt.subplots(2, 1, figsize=(10,6), sharex=True)\n",
    "\n",
    "axes[0].imshow(samples, cmap='viridis', aspect='auto')\n",
    "axes[0].set_ylabel('Trials id')\n",
    "axes[0].set_title('Stimulus')\n",
    "\n",
    "axes[1].imshow(targets, cmap='viridis', aspect='auto')\n",
    "axes[1].set_ylabel('Trials id')\n",
    "axes[1].set_title('Reward')\n",
    "axes[1].set_xlabel('Time')\n",
    "f.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_jSR_OE1LxHQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "outputId": "809fd75e-d62f-4160-8762-7a5729a99036"
   },
   "source": [
    "# Get Delta Values from Log\n",
    "delta_vals = np.squeeze(\n",
    "    prediction_error_mechanism.log.nparray_dictionary()[comp.name][pnl.VALUE]\n",
    ")\n",
    "\n",
    "t_start = 40\n",
    "x_vals, y_vals = np.meshgrid(\n",
    "    np.arange(n_trials), np.arange(t_start, n_time_steps))\n",
    "d_vals = delta_vals[:, t_start:n_time_steps].T\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(25, 80)\n",
    "ax.plot_surface(x_vals, y_vals, d_vals, linewidth=.5)\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"\\nTrial\")\n",
    "ax.set_ylabel(\"\\nTimestep\")\n",
    "ax.set_zlabel('\\n'+r\"$\\delta$\")\n",
    "ax.set_title(\"Montague et. al. (1996) -- Figure 5B\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqM9P4rbwQHT"
   },
   "source": [
    "This 3D figure contains a lot of information.  The z-axis is timesteps within a trial (time within trials is moving from the back \"wall\" toward the front of the graph).  Although we had 120 time steps in each trial, this figure is zoomed in on the action; between the onset of the stimulus at timestep 41 and the delivery of reward at timestep 54.  To first approximation the y-axis displays predictions and prediction errors -- positive values are predictions, and negative values are prediction errors (i.e. when a predicted reward was absent).  The x-axis displays all 120 trials of the experiment, progressing from left to right.  There are some prominent features in the graph.  On the back wall you can see the gradual positive build up (it looks a bit like a wave) of reward prediction accumulating at the onset of the stimulus (timestep 41, when the light turns on) -- the model is learning what it is supposed to!  Near the front left you can see predictions starting after the first trial with a reward, and you can see the negative spikes of prediction error on all the trials where reward was withheld.       \n",
    "\n",
    "## Figure 5C\n",
    "\n",
    "Finally, we'll recreate Figure 5C which demonstrates \"extinction of response to the sensory cue.\" After trial 70, we'll stop delivering reward so the system unlearns the response to the sensory cue. In order to show the full effect, this time we will run 150 trials."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-OPF8cQQLxHR"
   },
   "source": [
    "comp, nodes = get_model()\n",
    "[sample_mechanism, prediction_error_mechanism, target_mechanism] = nodes\n",
    "\n",
    "# Create Stimulus Dictionary\n",
    "n_trials = 150\n",
    "reward_removal_onset = 70\n",
    "no_reward_trials = np.arange(reward_removal_onset, n_trials)\n",
    "samples, targets = build_stimulus(n_trials, n_time_steps, no_reward_trials)\n",
    "\n",
    "# Run Composition\n",
    "input_dict = {sample_mechanism: samples, target_mechanism: targets}\n",
    "comp.learn(inputs=input_dict);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lIzzBIqaLxHR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "outputId": "51983447-d21e-43fe-ff01-e7d7becc76df"
   },
   "source": [
    "f, axes = plt.subplots(2, 1, figsize=(10,6), sharex=True)\n",
    "\n",
    "axes[0].imshow(samples, cmap='viridis', aspect='auto')\n",
    "axes[0].set_ylabel('Trials id')\n",
    "axes[0].set_title('Stimulus')\n",
    "\n",
    "axes[1].imshow(targets, cmap='viridis', aspect='auto')\n",
    "axes[1].set_ylabel('Trials id')\n",
    "axes[1].set_title('Reward')\n",
    "axes[1].set_xlabel('Time')\n",
    "f.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ekB8dO-MLxHR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "outputId": "f26d0aa2-93cf-4efa-ce19-e7307e2ff8ae"
   },
   "source": [
    "# Get Delta Values from Log\n",
    "delta_vals = np.squeeze(\n",
    "    prediction_error_mechanism.log.nparray_dictionary()[comp.name][pnl.VALUE]\n",
    ")\n",
    "\n",
    "t_start = 40\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x_vals, y_vals = np.meshgrid(\n",
    "    np.arange(n_trials), np.arange(t_start, n_time_steps))\n",
    "d_vals = delta_vals[:, t_start:n_time_steps].T\n",
    "ax.plot_surface(x_vals, y_vals, d_vals, linewidth=.5)\n",
    "ax.view_init(25, 275)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(\"\\nTrial\")\n",
    "ax.set_ylabel(\"\\nTimestep\")\n",
    "ax.set_zlabel('\\n'+r\"$\\delta$\")\n",
    "ax.set_title(\"Montague et. al. (1996) -- Figure 5C\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqsWPi2QwQHZ"
   },
   "source": [
    "# Lab Exercises\n",
    "\n",
    "## 1.  Your TD Model\n",
    "Create a new TD learning model with 60 trials and 40 timesteps. In every trial the stimulus should appear at timestep 10 and persist through the rest of the trial.  Reward appears at timestep 25 in all trials except trials 15, 25, 35, 45.  Run the model and create a figures comparable to 5A, plotting trials 1, 25, & 55.  Describe any noteworthy learning differences between the results in your figure and the original. \n",
    "\n",
    "\n",
    "Tips: Reuse the code from the model that produces figure 5A.  Create two variables, numTrials = 60, and numSteps = 40, then replace the 120's throughout the code with the appropriate variable names.  When you make the figure, change the title to \"(Your Last Name) TD Learning Model (2018)\" and change other labels to be accurate.    \n",
    "\n",
    "\n",
    "## 2.  Learning Rate\n",
    "Try modifying the learning rate in your TD model to see what happens.  Create three 5A style figures with three different learning rates: 0.1, 0.3, and 0.7.  (0.3 is the learning rate in the original model).  Update your figure titles to reflect the learning rate, and save your figures to include in your homework notebook.  Write a paragraph or two describing interesting features when comparing the different learning rates.  \n",
    "\n",
    "\n",
    "## 3.  Reward Variability  \n",
    "Remove the reward on every third trial and run your model again with a learning rate of 0.3.  Create a 3D figure like 5B, and include it in your homework notebook.  Describe these results and how you interpret them.       \n",
    "\n",
    "\n",
    "## 4. Stimulus Variability\n",
    "Your TD learning model gradually learns to associate the earliest stimulus that reliably predicts the reward.  Try adding a gap in the persistence of the stimulus at time step 15.  Here's some code to help     \n",
    "\n",
    "\n",
    "```{python}\n",
    "stimulus_onset = 10\n",
    "stimulus_gap = 15\n",
    "reward_delivery = 25\n",
    "\n",
    "\n",
    "samples = np.zeros(numSteps)\n",
    "samples[stimulus_onset:] = 1 # this sets all indices from stimulus_onset to the end to 1\n",
    "samples[stimulus_gap] = 0\n",
    "samples = np.tile(samples, (numTrials, 1)) # this creates a 2D array that replicates samples for all trials\n",
    "```\n",
    "\n",
    "Now the stimulus will onset at timestep 10, disappear at timestep 15, then reappear at timestep 16 and remain present for the rest of each trial.  \n",
    "\n",
    "Run the model, describe what happens, and explain why. \n",
    "\n",
    "\n",
    "## 5. B.U.I.L.D.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DtW7UJVaLxHS"
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}